# -*- coding: utf-8 -*-
"""prog_raffaele_diomaiuto_coding_avanzato

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VjcDuWElXzWTCqOwQ6jX5jlTpjcjq7vZ

La programmazione orientata agli oggetti è stata preferita per questo progetto perché offre:
Organizzazione: Raggruppa dati (statistiche, impostazioni) e funzioni (copia, elimina) in un'unica entità (SyncEase).
Scalabilità: Rende il codice più facile da estendere e aggiungere nuove funzionalità senza intaccare il resto.
"""

import os
import shutil
import multiprocessing
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import logging
from pathlib import Path
import tempfile

# Configurazione del Logging
# Il logging è fondamentale per un'applicazione robusta, permette di tracciare l'attività, diagnosticare problemi e monitorare le operazioni.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sync_log.txt'),
        logging.StreamHandler()
    ]
)

# Classe Principale SyncEase
# Incapsula la logica di sincronizzazione e gestisce le risorse concorrenti.
class SyncEase:
    def __init__(self, max_workers_threads=4, max_workers_processes=None):
        """
        Costruttore della classe syncEase.
        inizializza le impostazioni per il parallelismo e le statistiche della sessione.
        """
        self.max_workers_threads = max_workers_threads
        # max_workers_processes: Numero massimo di processi da usare per gestire batch di file. trovato su stackoverflow
        # Se None, viene impostato al numero di core CPU logici. I processi sono utili per distribuire il lavoro tra i core
        # e bypassare il Global Interpreter Lock (GIL) di Python, permettendo una vera concorrenza CPU-bound.
        self.max_workers_processes = max_workers_processes or multiprocessing.cpu_count()
        self.stats = {
            'files_copied': 0,
            'files_deleted': 0,
            'files_updated': 0,
            'errors': 0
        }

    def get_file_info(self, folder_path):
        """
        Raccoglie informazioni sui file (percorso relativo e timestamp di ultima modifica)
        all'interno di una cartella specificata e delle sue sottocartelle.
        Gestisce gli errori durante la scansione e li registra nel log.

        Ritorna:
            dict: Un dizionario dove le chiavi sono i percorsi relativi dei file
                  e i valori sono i loro timestamp di ultima modifica (float).
        """
        file_info = {} # Dizionario per memorizzare le informazioni sui file.
        local_errors = 0 # Contatore per gli errori all'interno di questa specifica funzione.
        try:
            # os.walk() scansiona ricorsivamente la directory, generando una tupla (root, dirs, files)
            # per ogni directory che visita. Usiamo '_' per 'dirs' perché non ci serve l'elenco delle sottodirectory qui.
            for root, _, files in os.walk(folder_path):
                for file in files: # Itera su ogni file nella directory corrente.
                    full_path = os.path.join(root, file) # Costruisce il percorso completo del file.
                    # os.path.relpath(): Calcola il percorso del file relativo alla 'folder_path'.
                    # Questo è cruciale per mantenere la struttura delle cartelle quando si sincronizzano i file.
                    relative_path = os.path.relpath(full_path, folder_path)
                    try:
                        # os.path.getmtime(): Ottiene il timestamp dell'ultima modifica del file.
                        # Questo timestamp è la base per determinare se un file è stato modificato e deve essere aggiornato.
                        file_info[relative_path] = os.path.getmtime(full_path)
                    except (OSError, IOError) as e:
                        # Cattura errori specifici del sistema operativo o di I/O (es. permessi negati, file inesistente/corrotto).
                        logging.error(f"Errore nel leggere il file {full_path}: {e}")
                        local_errors += 1
        except Exception as e:
            # Cattura errori più generici che potrebbero verificarsi durante l'esplorazione della directory (es. problemi di accesso alla cartella stessa).
            logging.error(f"Errore nell'esplorare la directory {folder_path}: {e}")
            local_errors += 1
        self.stats['errors'] += local_errors # Aggiorna il contatore globale degli errori della classe.
        return file_info # Restituisce il dizionario con le informazioni raccolte.

    def copy_file_safe(self, source_path, dest_path, relative_path, is_update=False):
        """
        Copia un singolo file in modo sicuro, gestendo la creazione delle directory di destinazione
        e gli errori durante l'operazione. Registra l'azione nel log.

        funz:
            source_path (str): Il percorso completo del file sorgente.
            dest_path (str): Il percorso completo dove copiare il file nella destinazione.
            relative_path (str): Il percorso relativo del file (per il logging).
            is_update (bool): True se l'operazione è un aggiornamento di un file esistente, False se è una nuova copia.

        Ritorna:
            tuple: (files_copied_increment, files_updated_increment, errors_increment)
        """
        copied_inc, updated_inc, errors_inc = 0, 0, 0 # Inizializza i contatori per questa specifica operazione.
        try:
            # os.makedirs(..., exist_ok=True): Crea ricorsivamente le directory necessarie nel percorso di destinazione.
            # exist_ok=True evita di generare un errore se le directory esistono già, rendendo la funzione robusta.
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            # shutil.copy2(): Copia il file sorgente alla destinazione, includendo i metadati del file
            # come i permessi e il timestamp di ultima modifica. Questo è cruciale per la sincronizzazione basata su timestamp.
            shutil.copy2(source_path, dest_path)
            if is_update: # Se è un aggiornamento, lo logga come tale.
                logging.info(f"Aggiornato: {relative_path}")
                updated_inc = 1
            else: # Altrimenti, è una nuova copia.
                logging.info(f"Copiato: {relative_path}")
                copied_inc = 1
        except (OSError, IOError, shutil.Error) as e:
            # Cattura errori legati al sistema operativo, I/O o specifici del modulo shutil (es. disco pieno, permessi negati).
            logging.error(f"Errore nella copia di {relative_path}: {e}")
            errors_inc = 1
        return copied_inc, updated_inc, errors_inc # Restituisce gli incrementi per le statistiche.

    def delete_file_safe(self, file_path, relative_path):
        """
        Elimina un singolo file in modo sicuro.
        Dopo l'eliminazione, tenta di rimuovere le directory genitore che potrebbero essere diventate vuote.
        Gestisce gli errori durante l'operazione.

        Parametri:
            file_path (str): Il percorso completo del file da eliminare.
            relative_path (str): Il percorso relativo del file (per il logging).

        Ritorna:
            tuple: (files_deleted_increment, errors_increment)
        """
        deleted_inc, errors_inc = 0, 0 # Inizializza i contatori per questa specifica operazione.
        try:
            if os.path.exists(file_path): # Controlla se il file esiste prima di tentare di rimuoverlo.
                os.remove(file_path) # Elimina il file.
                logging.info(f"Eliminato: {relative_path}")
                deleted_inc = 1
                dir_path = os.path.dirname(file_path) # Ottiene il percorso della directory che conteneva il file.
                try:
                    # os.removedirs(): Tenta di rimuovere ricorsivamente le directory vuote partendo da 'dir_path' e risalendo l'albero.
                    # Questo pulisce la struttura di destinazione dalle cartelle che non contengono più file.
                    os.removedirs(dir_path)
                except OSError:
                    # L'OSError qui è attesa se la directory non è vuota (es. contiene ancora altre sottocartelle o file non gestiti).
                    # Non è un vero errore di log in questo contesto, quindi lo ignoriamo con 'pass'.
                    pass
        except (OSError, IOError) as e:
            # Cattura errori legati al sistema operativo o I/O durante l'eliminazione del file.
            logging.error(f"Errore nell'eliminazione di {relative_path}: {e}")
            errors_inc = 1
        return deleted_inc, errors_inc # Restituisce gli incrementi per le statistiche.

    def process_file_batch(self, batch_data, source_folder, dest_folder, operation, max_workers_threads):
        """
        Processa un batch (sottogruppo) di file in parallelo usando un ThreadPoolExecutor.
        Gestisce operazioni di copia/aggiornamento o eliminazione per i file nel batch.

        Parametri:
            batch_data (list): Lista di tuple (relative_path, is_update) per 'copy', o solo relative_path per 'delete'.
            source_folder (str): Cartella sorgente principale.
            dest_folder (str): Cartella destinazione principale.
            operation (str): 'copy' o 'delete'.
            max_workers_threads (int): Numero massimo di thread da usare per questo batch.

        Ritorna:
            dict: Statistiche aggregate per il batch processato.
        """
        batch_stats = { # Statistiche specifiche per questo batch.
            'files_copied': 0,
            'files_deleted': 0,
            'files_updated': 0,
            'errors': 0
        }

        # ThreadPoolExecutor è scelto qui perché le operazioni di copia/eliminazione file sono principalmente I/O-bound.
        # Ciò significa che la maggior parte del tempo è spesa in attesa che il disco o la rete rispondano.
        # I thread sono molto efficienti in questi scenari perché permettono al programma di eseguire altre operazioni
        # mentre un thread è in attesa dell'I/O, aggirando parzialmente le limitazioni del Global Interpreter Lock (GIL) di Python per le operazioni di I/O.
        with ThreadPoolExecutor(max_workers=max_workers_threads) as executor:
            futures = []
            if operation == 'copy':
                for relative_path, is_update in batch_data:
                    source_path = os.path.join(source_folder, relative_path)
                    dest_path = os.path.join(dest_folder, relative_path)

                    future = executor.submit(
                        self.copy_file_safe,
                        source_path, dest_path, relative_path, is_update
                    )
                    futures.append(future)

                for future in futures:
                    copied, updated, errors = future.result()
                    batch_stats['files_copied'] += copied
                    batch_stats['files_updated'] += updated
                    batch_stats['errors'] += errors

            elif operation == 'delete':
                for relative_path in batch_data: # Itera sui file nel batch.
                    dest_path = os.path.join(dest_folder, relative_path) # Costruisce il percorso completo del file da eliminare.
                    # Invia la funzione 'delete_file_safe' per essere eseguita da un thread nel pool.
                    future = executor.submit(
                        self.delete_file_safe,
                        dest_path, relative_path
                    )
                    futures.append(future) # Aggiunge il Future alla lista.

                for future in futures: # Itera sui Future per raccogliere i risultati.
                    deleted, errors = future.result() # .result() blocca finché il task non è completato.
                    batch_stats['files_deleted'] += deleted # Aggiorna le statistiche del batch.
                    batch_stats['errors'] += errors

        return batch_stats # Restituisce le statistiche aggregate per il batch.

    def sync_folders(self, source_folder, destination_folder):
        """
        Funzione principale di sincronizzazione tra due cartelle.
        analisi dei file, determinazione delle modifiche e gestione parallela delle operazioni di copia/aggiornamento e eliminazione.
        """
        logging.info(f"Inizio sincronizzazione: {source_folder} -> {destination_folder}")
        start_time = time.time()

        if not os.path.exists(source_folder):
            logging.error(f"Cartella sorgente non trovata: {source_folder}")
            return False

        try:
            Path(destination_folder).mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logging.error(f"Impossibile creare la cartella di destinazione '{destination_folder}': {e}")
            return False # Fallisce se non riesce a creare la destinazione.

        session_stats = {
            'files_copied': 0,
            'files_deleted': 0,
            'files_updated': 0,
            'errors': 0
        }

        logging.info("Analisi dei file in corso...")
        source_files = self.get_file_info(source_folder)
        dest_files = self.get_file_info(destination_folder)
        files_to_copy_update = []
        files_to_delete = []

        # Logica per determinare i file da copiare e aggiornare
        for relative_path, source_mtime in source_files.items():
            if relative_path not in dest_files:
                # Il file non esiste nella destinazione: deve essere copiato.
                files_to_copy_update.append((relative_path, False)) # False indica che non è un aggiornamento, ma una nuova copia.
            elif source_mtime > dest_files[relative_path]:
                # Il file esiste nella destinazione, ma la versione sorgente è più recente (timestamp maggiore): deve essere aggiornato.
                files_to_copy_update.append((relative_path, True)) # True indica che è un aggiornamento.

        # logica per determinare i file da eliminare
        for relative_path in dest_files:
            if relative_path not in source_files:
                # Il file esiste nella destinazione ma non più nella sorgente: deve essere eliminato dalla destinazione.
                files_to_delete.append(relative_path)

        logging.info(f"File da copiare/aggiornare: {len(files_to_copy_update)}")
        logging.info(f"File da eliminare: {len(files_to_delete)}")

        def create_batches(file_list, num_processes):
            """
            Funzione interna per dividere una lista di file in sottogruppi (batch).
            Questo è utile per distribuire il lavoro tra i processi.
            """
            if not file_list:
                return []
            batch_size = max(1, len(file_list) // num_processes)
            # Crea una lista di liste (i batch) usando slicing.
            return [file_list[i:i + batch_size] for i in range(0, len(file_list), batch_size)]
        copy_batches = create_batches(files_to_copy_update, self.max_workers_processes)
        delete_batches = create_batches(files_to_delete, self.max_workers_processes)

        with ProcessPoolExecutor(max_workers=self.max_workers_processes) as executor:
            if copy_batches:
                logging.info("Avvio copia e aggiornamento file")
                copy_args = [ # Prepara gli argomenti per ogni chiamata a `process_file_batch` per la copia.
                    (batch, source_folder, destination_folder, 'copy', self.max_workers_threads)
                    for batch in copy_batches
                ]
                # executor.submit(): Invia ogni batch al pool di processi. Ogni submission avvia un nuovo processo (o ne riutilizza uno esistente nel pool).
                futures = [executor.submit(self.process_file_batch, *args) for args in copy_args]
                for future in futures:
                    result_stats = future.result()
                    session_stats['files_copied'] += result_stats['files_copied']
                    session_stats['files_updated'] += result_stats['files_updated']
                    session_stats['errors'] += result_stats['errors']

            if delete_batches:
                logging.info("Avvio eliminazione file obsoleti")
                delete_args = [
                    (batch, source_folder, destination_folder, 'delete', self.max_workers_threads)
                    for batch in delete_batches
                ]
                futures = [executor.submit(self.process_file_batch, *args) for args in delete_args]
                for future in futures:
                    result_stats = future.result()
                    session_stats['files_deleted'] += result_stats['files_deleted']
                    session_stats['errors'] += result_stats['errors']

        self.stats = session_stats

        end_time = time.time()
        logging.info(f"Sincronizzazione completata in {end_time - start_time:.2f} secondi.")
        return True # La sincronizzazione è stata eseguita con successo (anche se ci sono stati errori di singoli file).

    def print_stats(self):
        """
        Stampa le statistiche riepilogative della sincronizzazione a console.
        """
        print("\n Statistiche di sincronizzazione ")
        print(f"File copiati: {self.stats['files_copied']}")
        print(f"File aggiornati: {self.stats['files_updated']}")
        print(f"File eliminati: {self.stats['files_deleted']}")
        print(f"Errori riscontrati: {self.stats['errors']}")
        # Calcola il totale delle operazioni completate.
        total_ops = self.stats['files_copied'] + self.stats['files_updated'] + self.stats['files_deleted']
        print(f"Totale operazioni gestite: {total_ops}")

# Funzioni di Test per il Sistema di Sincronizzazione
# Queste funzioni sono progettate per creare un ambiente controllato e simulare diversi modi  di sincronizzazione.

def setup_test_folders(base_path):
    """
    Prepara una singola directory per i test, assicurandosi che sia pulita.
    elimina la cartella se esiste e la ricrea.

    Parametri:
        base_path (str): Il percorso della cartella da preparare.

    Ritorna:
        str: Il percorso della cartella pulita.
    """
    if os.path.exists(base_path):
        shutil.rmtree(base_path) # Elimina ricorsivamente la cartella e tutto il suo contenuto se esiste.
                                 # Questo garantisce un ambiente di test pulito ad ogni esecuzione.
    os.makedirs(base_path) # Crea la cartella.
    return base_path

def create_test_environment():
    """
    Crea un ambiente di test completo con diverse configurazioni di cartelle e file.
    Simula vari scenari reali per testare la robustezza del sistema di sincronizzazione.
    """
    test_cases = []
    base_dir = tempfile.mkdtemp() # Crea una directory temporanea principale.
                                  # Vantaggio: le directory temporanee vengono eliminate automaticamente dal sistema
                                  # alla chiusura del programma o del sistema operativo, garantendo pulizia.

    # --- Test Case 1: Pochi file ---
    # Scenario semplice per verificare le funzionalità base di copia e aggiornamento.
    test1_source = setup_test_folders(os.path.join(base_dir, "source_few"))
    test1_dest = setup_test_folders(os.path.join(base_dir, "destination_few"))
    with open(os.path.join(test1_source, "file1.txt"), "w") as f:
        f.write("Contenuto file 1 originale")
    with open(os.path.join(test1_source, "file2.txt"), "w") as f:
        f.write("Contenuto file 2")
    test_cases.append((test1_source, test1_dest, "Test 1: Pochi file"))

    # --- Test Case 2: Molti file ---
    # Scenario per testare le prestazioni con un volume più elevato di dati, dove il parallelismo dovrebbe brillare.
    test2_source = setup_test_folders(os.path.join(base_dir, "source_many"))
    test2_dest = setup_test_folders(os.path.join(base_dir, "destination_many"))
    for i in range(50): # Crea 50 file. Questo mette sotto stress il sistema e mostra i benefici del parallelismo.
        with open(os.path.join(test2_source, f"file_{i:03d}.txt"), "w") as f:
            f.write(f"Contenuto del file numero {i}")
    test_cases.append((test2_source, test2_dest, "Test 2: Molti file"))

    # --- Test Case 3: Sottocartelle ---
    # Scenario per assicurarsi che il sistema gestisca correttamente le strutture di directory annidate.
    test3_source = setup_test_folders(os.path.join(base_dir, "source_subdirs"))
    test3_dest = setup_test_folders(os.path.join(base_dir, "destination_subdirs"))
    # Crea una sottocartella e un file al suo interno.
    sub_dir = os.path.join(test3_source, "subfolder")
    os.makedirs(sub_dir)
    with open(os.path.join(sub_dir, "subfile.txt"), "w") as f:
        f.write("File nella sottocartella")
    # Aggiunge anche un file nella radice della sorgente per un test più completo.
    with open(os.path.join(test3_source, "root_file.txt"), "w") as f:
        f.write("File nella radice")
    test_cases.append((test3_source, test3_dest, "Test 3: Sottocartelle"))

    return test_cases # Restituisce la lista di tutti i casi di test configurati.

def run_full_tests():
    """
    Esegue un ciclo completo di test per il sistema di sincronizzazione.
    Inizializza l'ambiente, esegue le sincronizzazioni per ogni scenario di test
    e simula modifiche per verificare il comportamento del sistema nel tempo.
    """
    print(" Avvio del sistema per sincronizzare File SyncEase")

    test_cases = create_test_environment() # Prepara tutti gli ambienti di test.
    # Istanzia la classe SyncEase.
    sync_system = SyncEase(max_workers_threads=4, max_workers_processes=2)

    for source, dest, description in test_cases: # Itera su ogni scenario di test.
        print(f"\nScenario: {description}")
        print("test 1: sincronizzazione iniziale/copia ")
        start_time = time.time() # Tempo di inizio per questa fase di test.
        success = sync_system.sync_folders(source, dest) # Esegue la sincronizzazione.
        end_time = time.time() # Tempo di fine.

        if success:
            print(f"  Completato in: {end_time - start_time:.2f} secondi")
            sync_system.print_stats() # Stampa le statistiche della fase 1.
        else:
            print("  Sincronizzazione fallita in fase 1!")

        #  Test di modifica per il primo caso simulazione di cambiamenti nel tempo
        # Questa sezione simula un secondo ciclo di sincronizzazione dopo che i file sorgente sono stati modificati, aggiunti o eliminati.
        # È cruciale per verificare la capacità del sistema di gestire aggiornamenti e eliminazioni.
        if "test 1" in description: # Applica questa fase solo al "Test 1" (pochi file) per semplicità e chiarezza.
            print(f"\nScenario: {description}  fase 2: Modifica, aggiunta, eliminazione e risincronizzazione")

            time.sleep(0.1) # Breve pausa per assicurare che i timestamp di modifica (mtime) dei file siano diversi.
                            # Senza questa pausa, `os.path.getmtime` potrebbe restituire lo stesso valore per file modificati rapidamente.

            # Modifica un file esistente nella sorgente.
            file1_path = os.path.join(source, "file1.txt")
            with open(file1_path, "w") as f:
                f.write("Contenuto modificato del file 1 - nuovo")
            logging.info(f"Modificato: {os.path.basename(file1_path)}")

            # Aggiunge un nuovo file alla sorgente.
            file3_path = os.path.join(source, "file3.txt")
            with open(file3_path, "w") as f:
                f.write("Nuovo file aggiunto nel test di modifica")
            logging.info(f"Aggiunto: {os.path.basename(file3_path)}")

            # Elimina un file dalla sorgente.
            file2_path = os.path.join(source, "file2.txt")
            if os.path.exists(file2_path):
                os.remove(file2_path)
                logging.info(f"Eliminato {os.path.basename(file2_path)} dalla sorgente per test.")

            print("Eseguo la seconda sincronizzazione dopo le modifiche")
            start_time = time.time() # Tempo di inizio per la seconda sincronizzazione.
            success = sync_system.sync_folders(source, dest) # Esegue la risincronizzazione.
            end_time = time.time() # Tempo di fine.

            if success:
                print(f"Completato in (Fase 2): {end_time - start_time:.2f} secondi")
                sync_system.print_stats() # Stampa le statistiche aggiornate dopo la fase 2.
            else:
                print("  Sincronizzazione fallita per fase 2!")

# Questo blocco assicura che run_full_tests( venga eseguito solo quando lo script è avviato direttamente.
if __name__ == "__main__":
# multiprocessing.freeze_support(): Importante per le applicazioni che usano il modulo multiprocessing
    multiprocessing.freeze_support()
    try:
        run_full_tests() # Avvia l'esecuzione di tutti i test.
    except Exception as e:
        # Cattura qualsiasi eccezione non gestita a livello superiore per evitare che il programma crash.
        print(f"Errore critico durante l'esecuzione: {str(e)}")
        import traceback # Importa il modulo per stampare la traccia dello stack.
        traceback.print_exc() # Stampa la traccia completa dello stack, molto utile per il debugging.